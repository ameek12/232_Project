{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac50f86-e949-450f-856c-0b672749be78",
   "metadata": {},
   "source": [
    "# Downloading the OpenAQ Air Pollution dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301d986d-5986-4df0-999c-bbb348d35c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to download the Open AQ dataset from Amazon\n",
    "#!pip install awscli\n",
    "#!~/.local/bin/aws s3 cp --no-sign-request s3://openaq-data-archive/records/csv.gz/ dataset --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304b947-d52c-41fb-bcb3-b8431090e0a1",
   "metadata": {},
   "source": [
    "# Now that the download is over, let's do some Air Pollution Data Exploration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d2e48b-cac7-4dd6-a569-af548e5773a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19f5c66-3e3f-4026-9fce-d544ec282077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark session\n",
    "spark = SparkSession.builder.appName('232 Project').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f8da3b9-2d60-4fa9-bdc3-071083d6ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to put the unzipped csv files\n",
    "#!mkdir -p combined_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d202f1ae-7a0d-43fd-9408-f4f7fcfd26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find the .csv.gz files and unzip them then copy that to the folder\n",
    "#!find ./data/ -type f -name \"*.csv\" -exec gunzip -c {} \\; -exec cp {} combined_csv_files/ \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c59b605-4d99-4e0b-a268-9f8e3ff60d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!find ./combined_csv_files/ -type f -name \"*.csv.gz\" -exec gunzip -c {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77aa3c4b-5d32-4c9a-a93e-92155dc0f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /expanse/lustre/projects/uci150/ameek1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd2fed-63c6-47e4-a7e5-ed47c6cbface",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/expanse/lustre/projects/uci150/cmerry/data/records/csv.gz\"  \n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, DecimalType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"location_id\", IntegerType(), True),\n",
    "    StructField(\"sensors_id\", IntegerType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"datetime\", TimestampType(), True),\n",
    "    StructField(\"lat\", DecimalType(precision=10, scale=6), True),\n",
    "    StructField(\"lon\", DecimalType(precision=10, scale=6), True),\n",
    "    StructField(\"parameter\", StringType(), True),\n",
    "    StructField(\"units\", StringType(), True),\n",
    "    StructField(\"value\", DecimalType(precision=10, scale=6), True),\n",
    "])\n",
    "\n",
    "\n",
    "# Read CSV files matching the pattern into a DataFrame\n",
    "#df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(path)\n",
    "df = spark.read.option(\"recursiveFileLookup\", \"true\").csv(path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f832eb5-22aa-4687-b43e-022b2ee0ca6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameReader' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameReader' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf388d-ec7a-4f42-93ba-fed60fba9183",
   "metadata": {},
   "source": [
    "## Transforming the datetime col ie splitting the 'datetime' column into date and time and then dropping the 'datetime' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30d41d-4625-496d-919e-741fad2491c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, date_format, col\n",
    "df = df.withColumn('ut', unix_timestamp(col('datetime'), 'yyyy-MM-dd HH:mm:ss')) \\\n",
    "       .withColumn('dty', from_unixtime('ut')) \\\n",
    "       .withColumn('date', date_format('dty', 'yyyy-MM-dd')) \\\n",
    "       .withColumn('time', date_format('dty', 'HH:mm:ss'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c91f9a-39a8-421c-b6c1-230f94a5b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409822e8-b218-4646-93d9-c8b8da224acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0422ae-3cec-4123-9bed-ee9de8c7e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any null values? \n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select(*(count(when(col(c).isNull(), c)).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d4be1-5ca7-44d8-8307-c81baa30e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing if there are any rows where the value of the sensor measurement is below zero \n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "df.filter(func.col(\"value\") <= 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6caa7b2-8f79-4cb0-8e4b-b6d86b8b0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for sensor 1803 and parameter 'pm10'\n",
    "filtered_df = df.filter((df.sensors_id == 1803) & (df.parameter == \"pm10\")).orderBy(\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37509cb-c752-48d3-9bac-1de91e14a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting datetime and value for the graph\n",
    "pm10_data = filtered_df.select(\"datetime\", \"value\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4962d83-b900-4d82-b2c0-344d72bf3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting separate lists of dates and values\n",
    "dates = [data.datetime for data in pm10_data]\n",
    "values = [data.value for data in pm10_data]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(dates, values, marker='o', linestyle='-', color='b')\n",
    "plt.title(\"PM10 Levels Over Time for Sensor 1803 at Location 1000\")\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"PM10 Value (µg/m³)\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be0a68-a442-45fb-aade-c08ae2c5ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('pollution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437eccde-7a31-4e06-93bd-590f35b174e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the different number of parameters in the dataset\n",
    "results = spark.sql(\"SELECT count(distinct parameter) as Parameter_Count \\\n",
    "                    FROM pollution\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7a753-9fad-4736-b1c4-89a9c7e00452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the sensor counts for each location. \n",
    "results = spark.sql(\"SELECT location_id, sensors_id, count(sensors_id) as Sensor_Count \\\n",
    "                    FROM pollution \\\n",
    "                    GROUP BY location_id, sensors_id \\\n",
    "                    ORDER BY location_id\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d353c60-99fa-4492-9b87-66dabb63b9eb",
   "metadata": {},
   "source": [
    "# WHO air quality guidelines recommend a maximum exposure of 20 µg/m3 for PM10, 35 µg/m3 for PM2.5, and 0.070 parts per million for O3\n",
    "\n",
    "## Find the data in the maximum polluted thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974096a7-52fa-49c0-9d17-ab4de5f629eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the data in the polluted thresholds\n",
    "results = spark.sql(\"SELECT * \\\n",
    "                    FROM pollution \\\n",
    "                    WHERE (parameter = 'pm10' AND value > 20) OR (parameter = 'pm25' AND value > 12) OR (parameter = 'o3' AND value > 0.07)\")\n",
    "results.show()\n",
    "# Could gather insights about what the most polluted areas are "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0833aab-4075-4d38-a1ff-215f88f74414",
   "metadata": {},
   "source": [
    "### As we can see this occurs the same day around the same time period - ie. morning to midday "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02457cc1-8828-4e43-a62e-81b7d1b55c75",
   "metadata": {},
   "source": [
    "## Average Value per measurement for each year for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828ccfc-abc7-401d-8685-d29d3b0c8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT location_id, sensors_id, date, parameter, AVG(value) AS avg_value\\\n",
    "                    FROM pollution\\\n",
    "                    GROUP BY parameter, sensors_id, date, location_id\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ae436-1fae-4a51-941e-8c4c6e683d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423c80a-86df-4632-9daa-294cf6713485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
