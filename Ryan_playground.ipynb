{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAQ API\n",
    "\n",
    "- From the OpenAQ API, download a list of reference grade location_ids using the 'us' country code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# get reference grade sensors in the United States\n",
    "url = \"https://api.openaq.org/v2/locations?limit=4000&page=1&offset=0&sort=desc&country=US&order_by=lastUpdated&sensor_type=reference%20grade&dump_raw=false\"\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json'\n",
    "}\n",
    "\n",
    "api_data = requests.get(url, headers=headers).json()\n",
    "\n",
    "# get all location ids\n",
    "location_ids = []\n",
    "for i in api_data[\"results\"]:\n",
    "    if i['isMobile'] == False:\n",
    "        location_ids.append(i[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the OpenAQ data from AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "#----- Functions -----\n",
    "def download_and_process_files(key):\n",
    "    # download object from S3\n",
    "    # print(f'Downloading {key}')\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "\n",
    "    # unzip and store in dataframe\n",
    "    try:\n",
    "        with gzip.open(obj['Body'], 'rb') as f:\n",
    "            df = pd.read_csv(f)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error in Gzip process: {e}')\n",
    "# --------------------\n",
    "\n",
    "# OpenAQ bucket name and base path\n",
    "bucket_name = 'openaq-data-archive'\n",
    "base_s3_path = 'records/csv.gz'\n",
    "\n",
    "# location of CSV with location IDs\n",
    "location_ids_csv = './location_ids.csv'\n",
    "\n",
    "# parquet output path\n",
    "parquet_path = 'G:/_data'\n",
    "\n",
    "# number of workers used to download from S3 in parallel\n",
    "num_workers = 200\n",
    "\n",
    "location_ids = []\n",
    "\n",
    "with open(location_ids_csv, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        location_ids.append(row[0])\n",
    "\n",
    "for i in range(len(location_ids)):\n",
    "\n",
    "    location_id = location_ids[i]\n",
    "\n",
    "    # check for existing local file\n",
    "    if os.path.exists(f'{parquet_path}/{location_id}.parquet'):\n",
    "        print(f'Location {location_id} [{i + 1}/{len(location_ids)}] : Already downloaded')\n",
    "        continue\n",
    "\n",
    "    # set S3 path\n",
    "    s3_path = f'{base_s3_path}/locationid={location_id}'\n",
    "\n",
    "    # init pagination variables\n",
    "    continuation_token = None\n",
    "    s3_files = []\n",
    "\n",
    "    # paginate through the objects\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_path, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_path)\n",
    "\n",
    "        s3_files.extend([obj['Key'] for obj in response.get('Contents', [])])\n",
    "        \n",
    "        if 'NextContinuationToken' in response:\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # finished grabbing list of files for this location, print number of files\n",
    "    print(f'Location {location_id} [{i + 1}/{len(location_ids)}] : {len(s3_files)} files')\n",
    "\n",
    "    # download objects in parallel, using tqdm to track progress\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        dataframes = list(tqdm(executor.map(download_and_process_files, s3_files), total=len(s3_files)))\n",
    "\n",
    "    # write Parquet file\n",
    "    try:\n",
    "        parquet_file = f'{parquet_path}/{location_id}.parquet'\n",
    "        print(f'Writing Parquet file: {parquet_file}')\n",
    "        pd.concat(dataframes, ignore_index=True).to_parquet(parquet_file, engine='pyarrow', index=False)\n",
    "    except Exception as e:\n",
    "        print(f'Error writing Parquet file: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"6\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parquet files\n",
    "parquet_path = 'G:\\\\_data'\n",
    "df = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165618329"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: long (nullable = true)\n",
      " |-- sensors_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- units: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- lat.1: double (nullable = true)\n",
      " |-- lon.1: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unused columns\n",
    "\n",
    "Some source files have errant lat.1 and lon.1 columns. They are all NULL for all rows, and can be safely dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop lat.1 and lon.1\n",
    "df = df.drop('lat.1', 'lon.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nulls from chosen columns\n",
    "df = df.na.drop(subset=['location_id', 'location', 'value', 'parameter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with Value 0 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24453897"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(F.col(\"value\") <= 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only values > 0\n",
    "df = df.filter(F.col(\"value\") > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Latitude = Longitude\n",
    "\n",
    "Some rows are for locations that have the exact same longitude and latitude, which shouldn't be.\n",
    "\n",
    "The names of these places are often for locations not in the US. We'll filter them out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where lat and long are the same: 7353993\n",
      "+--------------+---------+---------+\n",
      "|      location|      lat|      lon|\n",
      "+--------------+---------+---------+\n",
      "|EH1F022-237357|4.3268808|4.3268808|\n",
      "+--------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find rows where latitude and longitude are the exact same\n",
    "count_lat_long_same = df.filter(F.col('lat') == F.col('lon')).count()\n",
    "print(f'Number of rows where lat and long are the same: {count_lat_long_same}')\n",
    "\n",
    "# example\n",
    "df.filter(F.col('lat') == F.col('lon')).select('location', 'lat', 'lon').limit(1).show()\n",
    "\n",
    "# filter out rows where lat and lon are equal\n",
    "df = df.filter(F.col('lat') != F.col('lon'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pollutants and their Units\n",
    "\n",
    "- we'll examine the pollutants and their units, and remove some we aren't interested in.\n",
    "- we'll also align things like the PM25's µg/m³ units:  µ (micro sign, Unicode: U+00B5) and μ (Greek letter mu, Unicode: U+03BC) are different characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+--------+\n",
      "|       parameter|        units|   count|\n",
      "+----------------+-------------+--------+\n",
      "|            pm25|        μg/m³|   19877|\n",
      "|             no2|          ppb|   82077|\n",
      "|              bc|        µg/m³|  151549|\n",
      "|relativehumidity|            %|  170299|\n",
      "|             voc|          iaq|  271634|\n",
      "|             nox|          ppm|  435455|\n",
      "|              no|          ppm|  475816|\n",
      "|           um100|particles/cm³|  545981|\n",
      "|             nox|        µg/m³| 1199185|\n",
      "|           um050|particles/cm³| 1267610|\n",
      "|              co|        µg/m³| 1317566|\n",
      "|              no|        µg/m³| 1574084|\n",
      "|     temperature|            c| 2322397|\n",
      "|             so2|        µg/m³| 2767543|\n",
      "|             so2|          ppm| 3047723|\n",
      "|              co|          ppm| 3137804|\n",
      "|           um025|particles/cm³| 3648657|\n",
      "|     temperature|            f| 3876648|\n",
      "|             pm1|        µg/m³| 5220020|\n",
      "|           um010|particles/cm³| 5548547|\n",
      "|             no2|          ppm| 5681196|\n",
      "|        pressure|           mb| 5697256|\n",
      "|        humidity|            %| 5711599|\n",
      "|              o3|        µg/m³| 5806486|\n",
      "|           um005|particles/cm³| 5835207|\n",
      "|           um003|particles/cm³| 6032708|\n",
      "|             no2|        µg/m³| 9492679|\n",
      "|              o3|          ppm|10932726|\n",
      "|            pm10|        µg/m³|18379782|\n",
      "|            pm25|        µg/m³|22847563|\n",
      "+----------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get unique parameters and their units\n",
    "df_pollutants = df.groupBy('parameter', 'units').count().sort('count')\n",
    "df_pollutants.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some variants of pm25 we aren't interested in\n",
    "df = df.filter((F.col('parameter') != 'pm25') | (F.col('units') != 'ppm'))\n",
    "\n",
    "# any units of 'ppb' have scant rows\n",
    "df = df.filter(F.col('units') != 'ppb')\n",
    "\n",
    "# any rows with units of 'particles/cm3' we aren't interested in\n",
    "df = df.filter(F.col('units') != 'particles/cm³')\n",
    "\n",
    "# filter out some other pollutants we're not interested in\n",
    "df = df.filter(~F.col('parameter').isin(['pm25-old', 'ch4', 'bc', 'nox', 'voc', 'no', 'co2', 'pm4']))\n",
    "\n",
    "# align ug/m3 units\n",
    "# NOTE: the string values below for ug/m3 should not be touched, retyped, etc. The micro/mu are specific characters that look the same, but are different\n",
    "df = df.withColumn('units', F.when(F.col('units') == 'μg/m³', 'µg/m³').otherwise(F.col('units')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Temperatures in C to F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert temperatures with units C to F\n",
    "df = df.withColumn('value',\n",
    "    F.when((F.col('parameter') == 'temperature') & (F.col('units') == 'c'),\n",
    "        (F.col('value') * 1.8) + 32)\n",
    "        .otherwise(F.col('value')\n",
    "    )\n",
    ")\n",
    "\n",
    "# change all units to F when parameter is 'temperature'\n",
    "df = df.withColumn('units', F.when(F.col('parameter') == 'temperature', 'f').otherwise(F.col('units')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Pressure Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hectopascal equals 1 millibar, so we'll just change the units to mb\n",
    "df = df.withColumn('units', F.when(F.col('units') == 'hpa', 'mb').otherwise(F.col('units')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert units for some pollutants\n",
    "\n",
    "**NOTE**: skipping this for now. Conversions vary depending on factors, and for now we'll concetrate on rows with the proper units\n",
    "\n",
    "- for some pollutants, there are measurements in both ug/m3 and ppm\n",
    "- the AQI calculator (EPA algorithm) expects that pollutants have certain units:\n",
    "  - CO - ppm *\n",
    "  - NO2 - ppb *\n",
    "  - O3 - ppm *\n",
    "  - PM10 - ug/m3\n",
    "  - PM25 - ug/m3\n",
    "  - SO2 - ppb *\n",
    "- starred units need conversion in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: from web research, the converions between ug/m3 and ppb can be quite different depending on temp and pressure, etc. For now, we'll just only use rows with the proper units.\n",
    "\n",
    "# get a few rows where parameter is 'co' and unit is 'ug/m3'\n",
    "# z = df.filter((F.col('parameter') == 'co') & (F.col('units') == 'µg/m³')).limit(5)\n",
    "\n",
    "# NOTE: For now, removing rows with improper units\n",
    "df = df.filter(~(F.col('parameter').isin(['co', 'no2', 'so2']) & (F.col('units') == 'µg/m³')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert no2 and so2 to ppb\n",
    "df = df.withColumn('value',\n",
    "    F.when(\n",
    "        (F.col('parameter').isin(['so2', 'no2'])) & (F.col('units') == 'ppm'),\n",
    "        F.col('value') * 1000)\n",
    "        .otherwise(F.col('value')\n",
    "    )\n",
    ")\n",
    "df = df.withColumn('units',\n",
    "    F.when(\n",
    "        (F.col('parameter').isin(['so2', 'no2'])) & (F.col('units') == 'ppm'),\n",
    "        'ppb')\n",
    "        .otherwise(F.col('units')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After this work on Parameters and Units, we'll create and cache the Pollutants for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       parameter|units|\n",
      "+----------------+-----+\n",
      "|              co|  ppm|\n",
      "|        humidity|    %|\n",
      "|             no2|  ppb|\n",
      "|              o3|  ppm|\n",
      "|              o3|µg/m³|\n",
      "|             pm1|µg/m³|\n",
      "|            pm10|µg/m³|\n",
      "|            pm25|µg/m³|\n",
      "|        pressure|   mb|\n",
      "|relativehumidity|    %|\n",
      "|             so2|  ppb|\n",
      "|     temperature|    f|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pollutants = df.groupBy('parameter', 'units').count().drop('count').sort('parameter', 'units').cache()\n",
    "df_pollutants.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll use rough Latitude / Longitude bounding box to limit to US stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude and longitude bounding box for US\n",
    "bounding_north = 50\n",
    "bounding_south = 24\n",
    "bounding_east = -66\n",
    "bounding_west = -125\n",
    "\n",
    "df = df.filter((F.col('lat') >= bounding_south) & (F.col('lat') <= bounding_north) & (F.col('lon') >= bounding_west) & (F.col('lon') <= bounding_east))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Averages\n",
    "\n",
    "- average rows per location, parameter, and day\n",
    "- i.e. average values for each unique combination of location, parameter, and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datetime column to yyyy-mm-dd as 'date'\n",
    "df = df.withColumn('date', F.date_format(F.col('datetime'), 'yyyy-MM-dd'))\n",
    "\n",
    "# as we're averaging on day, we don't need the precision of the datetime column\n",
    "df = df.drop('datetime')\n",
    "\n",
    "# also don't need sensors_id or units column anymore\n",
    "df = df.drop('sensors_id', 'units')\n",
    "\n",
    "df = df.groupBy('location_id', 'parameter', 'date').agg(\n",
    "    F.first('location').alias('location'),\n",
    "    F.first('lat').alias('lat'),\n",
    "    F.first('lon').alias('lon'),\n",
    "    F.avg('value').alias('value')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot Pollutants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot parameter column\n",
    "df = df.groupBy('location_id', 'location', 'date', 'lat', 'lon').pivot('parameter').agg(F.avg('value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop rows where PM25 is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where pm25 is null\n",
    "df = df.na.drop(subset=['pm25'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geocode\n",
    "\n",
    "We'll Geocode the latitude and longitude values to:\n",
    "\n",
    "- cull locations that aren't in the US\n",
    "- use the added categorical geographic information (State, City, etc) in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverse_geocoder as rg\n",
    "\n",
    "def geocode(locations_dict):\n",
    "    \n",
    "    # get list of lat, lon tuples from coords dict\n",
    "    coords = [(row['coords'][0], row['coords'][1]) for row in locations_dict.values()]\n",
    "\n",
    "    # reverse geocode\n",
    "    geocode_results = rg.search(coords)\n",
    "\n",
    "    geocode_results = [{'city': loc['name'], 'state': loc['admin1'], 'country_code': loc['cc']} for loc in geocode_results]\n",
    "    results = []\n",
    "    for (key, _), loc in zip(locations_dict.items(), geocode_results):\n",
    "        results.append((int(key), loc['city'], loc['state'], str.lower(loc['country_code'])))\n",
    "\n",
    "    return results\n",
    "\n",
    "# create a UDF with a map(string, string) return type (e.g. {country_code -> 'US'})\n",
    "geocode_udf = F.udf(geocode, T.MapType(T.StringType(), T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "# create dataframe of just distinct locations and their lat, lon\n",
    "locations = df.groupBy('location_id').agg(F.first('lat').alias('lat'), F.first('lon').alias('lon')).collect()\n",
    "\n",
    "# setup dictionary, where key is location_id, and value is a tuple of (lat, lon)\n",
    "locations = {row['location_id']: {'coords': (row['lat'], row['lon'])} for row in locations}\n",
    "\n",
    "# reverse geocode\n",
    "locations = geocode(locations)\n",
    "\n",
    "# create spark dataframe of results\n",
    "schema = T.StructType([\n",
    "    T.StructField('location_id', T.LongType(), True),\n",
    "    T.StructField('city', T.StringType(), True),\n",
    "    T.StructField('state', T.StringType(), True),\n",
    "    T.StructField('country_code', T.StringType(), True)\n",
    "])\n",
    "geocoded_df = spark.createDataFrame(locations, schema=schema)\n",
    "\n",
    "# join this dataframe with the original dataframe\n",
    "df = df.join(geocoded_df, on='location_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows that aren't in the United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter((F.col('country_code')) == 'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQI Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aqi\n",
    "\n",
    "def calculate_aqi(pm10, pm25, co, no2, o3, so2):\n",
    "\n",
    "    try:\n",
    "        pollutants = []\n",
    "\n",
    "        pollutants.append((aqi.POLLUTANT_PM10, pm10)) if pm10 else None\n",
    "        pollutants.append((aqi.POLLUTANT_PM25, pm25)) if pm25 else None\n",
    "        pollutants.append((aqi.POLLUTANT_CO_8H, co)) if co else None\n",
    "        pollutants.append((aqi.POLLUTANT_NO2_1H, no2)) if no2 else None\n",
    "        pollutants.append((aqi.POLLUTANT_O3_8H, o3)) if o3 else None\n",
    "        pollutants.append((aqi.POLLUTANT_SO2_1H, so2)) if so2 else None\n",
    "\n",
    "        return(float(aqi.to_aqi(pollutants)))\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "# -------------------------------\n",
    "\n",
    "# register UDF\n",
    "aqi_udf = F.udf(calculate_aqi, T.FloatType())\n",
    "\n",
    "# add AQI column\n",
    "df = df.withColumn('aqi', aqi_udf(F.col('pm10'), F.col('pm25'), F.col('co'), F.col('no2'), F.col('o3'), F.col('so2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aqi_category(aqi_score):\n",
    "    if aqi_score is None:\n",
    "        return None\n",
    "    elif aqi_score <= 50:\n",
    "        return \"Good\"\n",
    "    elif aqi_score <= 100:\n",
    "        return \"Moderate\"\n",
    "    elif aqi_score <= 150:\n",
    "        return \"Unhealthy for Sensitive Groups\"\n",
    "    elif aqi_score <= 200:\n",
    "        return \"Unhealthy\"\n",
    "    elif aqi_score <= 300:\n",
    "        return \"Very Unhealthy\"\n",
    "    elif aqi_score > 300:\n",
    "        return \"Hazardous\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# register UDF\n",
    "aqi_category_udf = F.udf(aqi_category, T.StringType())\n",
    "\n",
    "df = df.withColumn(\"aqi_category\", aqi_category_udf(F.col(\"aqi\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at Average PM2.5 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of average pm25 by date. this is averaged over all locations\n",
    "df_pm25 = df.groupBy('date').agg(F.avg('pm25').alias('pm25'))\n",
    "_pm25 = df_pm25.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "_pm25['date'] = pd.to_datetime(_pm25['date'])\n",
    "\n",
    "# sort by date\n",
    "_pm25 = _pm25.sort_values('date')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(_pm25['date'], _pm25['pm25'], marker='o', linestyle='-')\n",
    "\n",
    "plt.title('PM2.5 Values in US')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average PM2.5 (µg/m³)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into  training and tests sets, create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "df_model = df\n",
    "\n",
    "# feature engineering: extract date parts\n",
    "df_model = df_model.withColumn(\"year\", F.year(\"date\"))\n",
    "df_model = df_model.withColumn(\"month\", F.month(\"date\"))\n",
    "df_model = df_model.withColumn(\"day\", F.dayofmonth(\"date\"))\n",
    "df_model = df_model.withColumn(\"day_of_week\", F.dayofweek(\"date\"))\n",
    "\n",
    "# creating feature vectors\n",
    "assembler = VectorAssembler(inputCols=[\"year\", \"month\", \"day\", \"day_of_week\"], outputCol=\"features\")\n",
    "df_model = assembler.transform(df_model)\n",
    "\n",
    "# split into test and training sets based on 80/20\n",
    "train_df, test_df = df_model.randomSplit([0.8, 0.2])\n",
    "\n",
    "# split into test and training sets based on date\n",
    "# train_df = df_model.filter(F.col('date') < '2023-01-01')\n",
    "# test_df = df_model.filter(F.col('date') >= '2023-01-01')\n",
    "\n",
    "# create a linear regression model\n",
    "lr_model = LinearRegression(featuresCol='features', labelCol='pm25').fit(train_df)\n",
    "\n",
    "# print the coefficients and intercept for linear regression\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df\n",
    "\n",
    "# converting date column to timestamp \n",
    "df_model = df.withColumn('timestamp', F.unix_timestamp(F.col('date'), 'yyyy-MM-dd'))\n",
    "\n",
    "# creating feature vectors\n",
    "assembler = VectorAssembler(inputCols=[\"timestamp\"], outputCol=\"features\")\n",
    "df_model = assembler.transform(df_model)\n",
    "\n",
    "# split into test and training sets based on date\n",
    "train_df = df_model.filter(F.col('date') < '2023-01-01')\n",
    "test_df = df_model.filter(F.col('date') >= '2023-01-01')\n",
    "\n",
    "# create a linear regression model\n",
    "lr_model = LinearRegression(featuresCol='features', labelCol='pm25').fit(train_df)\n",
    "\n",
    "# print the coefficients and intercept for linear regression\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on training data\n",
    "train_predictions = lr_model.transform(train_df)\n",
    "\n",
    "# predictions on testing data\n",
    "test_predictions = lr_model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"pm25\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# RMSE for training\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "\n",
    "# RMSE for testing\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Testing RMSE:\", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "Our model fits in the underfitting side of the fitting graph. Our linear regression model is on the more simple side, with only one parameter reading vs. timestamp as the input of trying to predict future values. We started off with pm values across many stations, but averaged these across the entire US and across each day as an input to our model, which simplified the model. Unforunately, the values in our dataset only go a couple of years back. Because it is more simple due to these reasons, the model is prone to underfitting and not being as robust as if we had added more time points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Models\n",
    "\n",
    "The next models we are thinking of will utilize more of the parameters we have in our dataset. We can apply k-means clustering to obtain clusters of places with similar levels/patterns of pollution based on features. This would be interesting to investigate as we could look into areas of best and worst pollution in the US. We could also run PCA on these parameter features for dimensionality reduction on our dataset as we have many parameters of data to pool from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
