{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAQ API\n",
    "\n",
    "- From the OpenAQ API, download a list of reference grade location_ids using the 'us' country code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# get reference grade sensors in the United States\n",
    "url = \"https://api.openaq.org/v2/locations?limit=4000&page=1&offset=0&sort=desc&country=US&order_by=lastUpdated&sensor_type=reference%20grade&dump_raw=false\"\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json'\n",
    "}\n",
    "\n",
    "api_data = requests.get(url, headers=headers).json()\n",
    "\n",
    "# get all location ids\n",
    "location_ids = []\n",
    "for i in api_data[\"results\"]:\n",
    "    if i['isMobile'] == False:\n",
    "        location_ids.append(i[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the OpenAQ data from AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "#----- Functions -----\n",
    "def download_and_process_files(key):\n",
    "    # download object from S3\n",
    "    # print(f'Downloading {key}')\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "\n",
    "    # unzip and store in dataframe\n",
    "    try:\n",
    "        with gzip.open(obj['Body'], 'rb') as f:\n",
    "            df = pd.read_csv(f)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error in Gzip process: {e}')\n",
    "# --------------------\n",
    "\n",
    "# OpenAQ bucket name and base path\n",
    "bucket_name = 'openaq-data-archive'\n",
    "base_s3_path = 'records/csv.gz'\n",
    "\n",
    "# location of CSV with location IDs\n",
    "location_ids_csv = './location_ids.csv'\n",
    "\n",
    "# parquet output path\n",
    "parquet_path = 'G:/_data'\n",
    "\n",
    "# number of workers used to download from S3 in parallel\n",
    "num_workers = 200\n",
    "\n",
    "location_ids = []\n",
    "\n",
    "with open(location_ids_csv, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        location_ids.append(row[0])\n",
    "\n",
    "for i in range(len(location_ids)):\n",
    "\n",
    "    location_id = location_ids[i]\n",
    "\n",
    "    # check for existing local file\n",
    "    if os.path.exists(f'{parquet_path}/{location_id}.parquet'):\n",
    "        print(f'Location {location_id} [{i + 1}/{len(location_ids)}] : Already downloaded')\n",
    "        continue\n",
    "\n",
    "    # set S3 path\n",
    "    s3_path = f'{base_s3_path}/locationid={location_id}'\n",
    "\n",
    "    # init pagination variables\n",
    "    continuation_token = None\n",
    "    s3_files = []\n",
    "\n",
    "    # paginate through the objects\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_path, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_path)\n",
    "\n",
    "        s3_files.extend([obj['Key'] for obj in response.get('Contents', [])])\n",
    "        \n",
    "        if 'NextContinuationToken' in response:\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # finished grabbing list of files for this location, print number of files\n",
    "    print(f'Location {location_id} [{i + 1}/{len(location_ids)}] : {len(s3_files)} files')\n",
    "\n",
    "    # download objects in parallel, using tqdm to track progress\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        dataframes = list(tqdm(executor.map(download_and_process_files, s3_files), total=len(s3_files)))\n",
    "\n",
    "    # write Parquet file\n",
    "    try:\n",
    "        parquet_file = f'{parquet_path}/{location_id}.parquet'\n",
    "        print(f'Writing Parquet file: {parquet_file}')\n",
    "        pd.concat(dataframes, ignore_index=True).to_parquet(parquet_file, engine='pyarrow', index=False)\n",
    "    except Exception as e:\n",
    "        print(f'Error writing Parquet file: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-wrcetkoo because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# SET: use to the values used to start this SDSC Jupyter session\n",
    "node_memory = 80\n",
    "node_cores = 40\n",
    "\n",
    "# SET: how many cores you want allocated to the driver node\n",
    "driver_cores = 10\n",
    "\n",
    "\n",
    "executor_cores = node_cores - driver_cores\n",
    "executor_memory = node_memory // executor_cores\n",
    "driver_memory = node_memory - (executor_cores * executor_memory)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", f'{driver_memory}g') \\\n",
    "    .config(\"spark.executor.memory\", f'{executor_memory}g')\\\n",
    "    .config('spark.executor.instances', executor_cores) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parquet files\n",
    "parquet_path = \"/expanse/lustre/projects/uci150/slin17/project/parquet_data\"\n",
    "df = spark.read.parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941941144"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: long (nullable = true)\n",
      " |-- sensors_id: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lat.1: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lon.1: double (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- units: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unused columns\n",
    "\n",
    "Some source files have errant lat.1 and lon.1 columns. They are all NULL for all rows, and can be safely dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop lat.1 and lon.1\n",
    "df = df.drop('lat.1', 'lon.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nulls from chosen columns\n",
    "df = df.na.drop(subset=['location_id', 'location', 'value', 'parameter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with Value 0 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86980986"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(F.col(\"value\") <= 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only values > 0\n",
    "df = df.filter(F.col(\"value\") > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Latitude = Longitude\n",
    "\n",
    "Some rows are for locations that have the exact same longitude and latitude, which shouldn't be.\n",
    "\n",
    "The names of these places are often for locations not in the US. We'll filter them out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where lat and long are the same: 11833496\n",
      "+------------------+---------+---------+\n",
      "|          location|      lat|      lon|\n",
      "+------------------+---------+---------+\n",
      "|Essai 25/01-289045|2.3923989|2.3923989|\n",
      "+------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find rows where latitude and longitude are the exact same\n",
    "count_lat_long_same = df.filter(F.col('lat') == F.col('lon')).count()\n",
    "print(f'Number of rows where lat and long are the same: {count_lat_long_same}')\n",
    "\n",
    "# example\n",
    "df.filter(F.col('lat') == F.col('lon')).select('location', 'lat', 'lon').limit(1).show()\n",
    "\n",
    "# filter out rows where lat and lon are equal\n",
    "df = df.filter(F.col('lat') != F.col('lon'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pollutants and their Units\n",
    "\n",
    "- we'll examine the pollutants and their units, and remove some we aren't interested in.\n",
    "- we'll also align things like the PM25's µg/m³ units:  µ (micro sign, Unicode: U+00B5) and μ (Greek letter mu, Unicode: U+03BC) are different characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+---------+\n",
      "|       parameter|        units|    count|\n",
      "+----------------+-------------+---------+\n",
      "|            pm25|          ppm|      258|\n",
      "|        pm25-old|         ugm3|      299|\n",
      "|             so2|          ppb|     3582|\n",
      "|           ozone|          ppb|     5130|\n",
      "|              co|          ppb|     5314|\n",
      "|             ch4|          ppm|    13944|\n",
      "|             no2|          ppb|    14095|\n",
      "|             ufp|particles/cm³|    38251|\n",
      "|            pm25|        μg/m³|   296979|\n",
      "|           um100|particles/cm³|  1477086|\n",
      "|              bc|        µg/m³|  1542442|\n",
      "|             nox|          ppm|  2355519|\n",
      "|             voc|          iaq|  2456219|\n",
      "|           um050|particles/cm³|  3883017|\n",
      "|              no|        µg/m³|  4341888|\n",
      "|             nox|        µg/m³|  6072488|\n",
      "|              no|          ppm|  7063385|\n",
      "|             co2|          ppm|  8605548|\n",
      "|relativehumidity|            %|  9233362|\n",
      "|             so2|          ppm| 10652117|\n",
      "|        pressure|          hpa| 10796381|\n",
      "|           um025|particles/cm³| 11041377|\n",
      "|              co|          ppm| 13728314|\n",
      "|           um010|particles/cm³| 16608258|\n",
      "|           um005|particles/cm³| 17253363|\n",
      "|           um003|particles/cm³| 17686584|\n",
      "|             pm4|        µg/m³| 18438415|\n",
      "|        pressure|           mb| 20157404|\n",
      "|     temperature|            f| 24983240|\n",
      "|             so2|        µg/m³| 26957661|\n",
      "|              co|        µg/m³| 27230166|\n",
      "|             no2|          ppm| 28698791|\n",
      "|     temperature|            c| 30760152|\n",
      "|        humidity|            %| 30916449|\n",
      "|              o3|        µg/m³| 35201703|\n",
      "|             no2|        µg/m³| 48683546|\n",
      "|             pm1|        µg/m³| 49363091|\n",
      "|              o3|          ppm| 54385022|\n",
      "|            pm10|        µg/m³|111150432|\n",
      "|            pm25|        µg/m³|140687303|\n",
      "+----------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get unique parameters and their units\n",
    "df_pollutants = df.groupBy('parameter', 'units').count().sort('count')\n",
    "df_pollutants.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some variants of pm25 we aren't interested in\n",
    "df = df.filter((F.col('parameter') != 'pm25') | (F.col('units') != 'ppm'))\n",
    "\n",
    "# any units of 'ppb' have scant rows\n",
    "df = df.filter(F.col('units') != 'ppb')\n",
    "\n",
    "# any rows with units of 'particles/cm3' we aren't interested in\n",
    "df = df.filter(F.col('units') != 'particles/cm³')\n",
    "\n",
    "# filter out some other pollutants we're not interested in\n",
    "df = df.filter(~F.col('parameter').isin(['pm25-old', 'ch4', 'bc', 'nox', 'voc', 'no', 'co2', 'pm4']))\n",
    "\n",
    "# align ug/m3 units\n",
    "# NOTE: the string values below for ug/m3 should not be touched, retyped, etc. The micro/mu are specific characters that look the same, but are different\n",
    "df = df.withColumn('units', F.when(F.col('units') == 'μg/m³', 'µg/m³').otherwise(F.col('units')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Temperatures in C to F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert temperatures with units C to F\n",
    "df = df.withColumn('value',\n",
    "    F.when((F.col('parameter') == 'temperature') & (F.col('units') == 'c'),\n",
    "        (F.col('value') * 1.8) + 32)\n",
    "        .otherwise(F.col('value')\n",
    "    )\n",
    ")\n",
    "\n",
    "# change all units to F when parameter is 'temperature'\n",
    "df = df.withColumn('units', F.when(F.col('parameter') == 'temperature', 'f').otherwise(F.col('units')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Pressure Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hectopascal equals 1 millibar, so we'll just change the units to mb\n",
    "df = df.withColumn('units', F.when(F.col('units') == 'hpa', 'mb').otherwise(F.col('units')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert units for some pollutants\n",
    "\n",
    "**NOTE**: skipping this for now. Conversions vary depending on factors, and for now we'll concetrate on rows with the proper units\n",
    "\n",
    "- for some pollutants, there are measurements in both ug/m3 and ppm\n",
    "- the AQI calculator (EPA algorithm) expects that pollutants have certain units:\n",
    "  - CO - ppm *\n",
    "  - NO2 - ppb *\n",
    "  - O3 - ppm *\n",
    "  - PM10 - ug/m3\n",
    "  - PM25 - ug/m3\n",
    "  - SO2 - ppb *\n",
    "- starred units need conversion in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: from web research, the converions between ug/m3 and ppb can be quite different depending on temp and pressure, etc. For now, we'll just only use rows with the proper units.\n",
    "\n",
    "# get a few rows where parameter is 'co' and unit is 'ug/m3'\n",
    "# z = df.filter((F.col('parameter') == 'co') & (F.col('units') == 'µg/m³')).limit(5)\n",
    "\n",
    "# NOTE: For now, removing rows with improper units\n",
    "df = df.filter(~(F.col('parameter').isin(['co', 'no2', 'so2']) & (F.col('units') == 'µg/m³')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert no2 and so2 to ppb\n",
    "df = df.withColumn('value',\n",
    "    F.when(\n",
    "        (F.col('parameter').isin(['so2', 'no2'])) & (F.col('units') == 'ppm'),\n",
    "        F.col('value') * 1000)\n",
    "        .otherwise(F.col('value')\n",
    "    )\n",
    ")\n",
    "df = df.withColumn('units',\n",
    "    F.when(\n",
    "        (F.col('parameter').isin(['so2', 'no2'])) & (F.col('units') == 'ppm'),\n",
    "        'ppb')\n",
    "        .otherwise(F.col('units')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After this work on Parameters and Units, we'll create and cache the Pollutants for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       parameter|units|\n",
      "+----------------+-----+\n",
      "|              co|  ppm|\n",
      "|        humidity|    %|\n",
      "|             no2|  ppb|\n",
      "|              o3|  ppm|\n",
      "|              o3|µg/m³|\n",
      "|             pm1|µg/m³|\n",
      "|            pm10|µg/m³|\n",
      "|            pm25|µg/m³|\n",
      "|        pressure|   mb|\n",
      "|relativehumidity|    %|\n",
      "|             so2|  ppb|\n",
      "|     temperature|    f|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pollutants = df.groupBy('parameter', 'units').count().drop('count').sort('parameter', 'units').cache()\n",
    "df_pollutants.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll use rough Latitude / Longitude bounding box to limit to US stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude and longitude bounding box for US\n",
    "bounding_north = 50\n",
    "bounding_south = 24\n",
    "bounding_east = -66\n",
    "bounding_west = -125\n",
    "\n",
    "#df = df.filter((F.col('lat') >= bounding_south) & (F.col('lat') <= bounding_north) & (F.col('lon') >= bounding_west) & (F.col('lon') <= bounding_east))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Averages\n",
    "\n",
    "- average rows per location, parameter, and day\n",
    "- i.e. average values for each unique combination of location, parameter, and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datetime column to yyyy-mm-dd as 'date'\n",
    "df = df.withColumn('date', F.date_format(F.col('datetime'), 'yyyy-MM-dd'))\n",
    "\n",
    "# as we're averaging on day, we don't need the precision of the datetime column\n",
    "df = df.drop('datetime')\n",
    "\n",
    "# also don't need sensors_id or units column anymore\n",
    "df = df.drop('sensors_id', 'units')\n",
    "\n",
    "df = df.groupBy('location_id', 'parameter', 'date').agg(\n",
    "    F.first('location').alias('location'),\n",
    "    F.first('lat').alias('lat'),\n",
    "    F.first('lon').alias('lon'),\n",
    "    F.avg('value').alias('value')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot Pollutants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot parameter column\n",
    "df = df.groupBy('location_id', 'location', 'date', 'lat', 'lon').pivot('parameter').agg(F.avg('value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop rows where PM25 is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where pm25 is null\n",
    "df = df.na.drop(subset=['pm25'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geocode\n",
    "\n",
    "We'll Geocode the latitude and longitude values to:\n",
    "\n",
    "- cull locations that aren't in the US\n",
    "- use the added categorical geographic information (State, City, etc) in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting reverse_geocoder\n",
      "  Downloading reverse_geocoder-1.5.1.tar.gz (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from reverse_geocoder) (1.24.4)\n",
      "Requirement already satisfied: scipy>=0.17.1 in /opt/conda/lib/python3.11/site-packages (from reverse_geocoder) (1.11.3)\n",
      "Building wheels for collected packages: reverse_geocoder\n",
      "  Building wheel for reverse_geocoder (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for reverse_geocoder: filename=reverse_geocoder-1.5.1-py3-none-any.whl size=2268064 sha256=e0bc57845e7ca1ad03b1773237a1f63eaf95ec1a7fddb5345ff8bf7238f5ce42\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_3eu1pao/wheels/17/3c/41/2bc89719586c2a5c53e9a527daa76a968a1288315c1ae2d904\n",
      "Successfully built reverse_geocoder\n",
      "Installing collected packages: reverse_geocoder\n",
      "Successfully installed reverse_geocoder-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install reverse_geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverse_geocoder as rg\n",
    "\n",
    "def geocode(locations_dict):\n",
    "    \n",
    "    # get list of lat, lon tuples from coords dict\n",
    "    coords = [(row['coords'][0], row['coords'][1]) for row in locations_dict.values()]\n",
    "\n",
    "    # reverse geocode\n",
    "    geocode_results = rg.search(coords)\n",
    "\n",
    "    geocode_results = [{'city': loc['name'], 'state': loc['admin1'], 'country_code': loc['cc']} for loc in geocode_results]\n",
    "    results = []\n",
    "    for (key, _), loc in zip(locations_dict.items(), geocode_results):\n",
    "        results.append((int(key), loc['city'], loc['state'], str.lower(loc['country_code'])))\n",
    "\n",
    "    return results\n",
    "\n",
    "# create a UDF with a map(string, string) return type (e.g. {country_code -> 'US'})\n",
    "geocode_udf = F.udf(geocode, T.MapType(T.StringType(), T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "# create dataframe of just distinct locations and their lat, lon\n",
    "locations = df.groupBy('location_id').agg(F.first('lat').alias('lat'), F.first('lon').alias('lon')).collect()\n",
    "\n",
    "# setup dictionary, where key is location_id, and value is a tuple of (lat, lon)\n",
    "locations = {row['location_id']: {'coords': (row['lat'], row['lon'])} for row in locations}\n",
    "\n",
    "# reverse geocode\n",
    "locations = geocode(locations)\n",
    "\n",
    "# create spark dataframe of results\n",
    "schema = T.StructType([\n",
    "    T.StructField('location_id', T.LongType(), True),\n",
    "    T.StructField('city', T.StringType(), True),\n",
    "    T.StructField('state', T.StringType(), True),\n",
    "    T.StructField('country_code', T.StringType(), True)\n",
    "])\n",
    "geocoded_df = spark.createDataFrame(locations, schema=schema)\n",
    "\n",
    "# join this dataframe with the original dataframe\n",
    "df = df.join(geocoded_df, on='location_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location_id',\n",
       " 'location',\n",
       " 'date',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'co',\n",
       " 'humidity',\n",
       " 'no2',\n",
       " 'o3',\n",
       " 'pm1',\n",
       " 'pm10',\n",
       " 'pm25',\n",
       " 'pressure',\n",
       " 'relativehumidity',\n",
       " 'so2',\n",
       " 'temperature',\n",
       " 'city',\n",
       " 'state',\n",
       " 'country_code']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows that aren't in the United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter((F.col('country_code')) == 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+---------+-----------+--------------------+--------+------------------+--------------------+----+------------------+------------------+--------+----------------+------------------+-----------+---------------+-------------+------------+\n",
      "|location_id|            location|      date|      lat|        lon|                  co|humidity|               no2|                  o3| pm1|              pm10|              pm25|pressure|relativehumidity|               so2|temperature|           city|        state|country_code|\n",
      "+-----------+--------------------+----------+---------+-----------+--------------------+--------+------------------+--------------------+----+------------------+------------------+--------+----------------+------------------+-----------+---------------+-------------+------------+\n",
      "|        186|Houston Aldine C8...|2023-05-10|  29.9011|   -95.3261|                NULL|    NULL|              NULL|0.025708333333333333|NULL|              NULL|             7.625|    NULL|            NULL|              NULL|       NULL|         Aldine|        Texas|          us|\n",
      "|        239|      Alamo Lake-239|2022-03-17|34.243889|-113.558611|                NULL|    NULL|              NULL|               0.046|NULL|               9.6|              4.25|    NULL|            NULL|              NULL|       NULL|         Salome|      Arizona|          us|\n",
      "|        259|          Troost-259|2022-10-18| 39.10465|  -94.57055|                NULL|    NULL|11.013636363636364|                NULL|NULL| 34.89090909090909|2.6899999999999995|    NULL|            NULL|1.5818181818181818|       NULL|    Kansas City|     Missouri|          us|\n",
      "|        274| Reading Airport-274|2023-10-14|  40.3833|   -75.9686|                NULL|    NULL|              NULL|0.008304347826086956|NULL|              NULL| 4.683333333333333|    NULL|            NULL|              NULL|       NULL|River View Park| Pennsylvania|          us|\n",
      "|        288|       Hawthorne-288|2019-09-10|40.733501|-111.871696| 0.11666666666666665|    NULL|12.714285714285714| 0.04671428571428571|NULL|              NULL|18.085714285714285|    NULL|            NULL|               1.0|       NULL|South Salt Lake|         Utah|          us|\n",
      "|        288|       Hawthorne-288|2017-10-20|40.733501|-111.871696|  0.2820000000000001|    NULL|              19.1|0.022500000000000003|NULL|              NULL| 8.919999999999998|    NULL|            NULL|               1.0|       NULL|South Salt Lake|         Utah|          us|\n",
      "|        331|          Lindon-331|2023-01-29|  40.3414|  -111.7136|  0.3166666666666666|    NULL| 7.670833333333334|0.031250000000000014|NULL|               5.0|1.9208333333333334|    NULL|            NULL|              NULL|       NULL|         Lindon|         Utah|          us|\n",
      "|        342|       Lancaster-342|2019-07-04|  40.0467|   -76.2833|                NULL|    NULL|              NULL|0.028521739130434785|NULL|              NULL|           11.9125|    NULL|            NULL|              NULL|       NULL|      Lancaster| Pennsylvania|          us|\n",
      "|        344|   Sunrise Acres-344|2021-07-01|36.163962| -115.11393| 0.11727272727272725|    NULL|11.545454545454545|                NULL|NULL|28.454545454545453| 7.722727272727272|    NULL|            NULL|              NULL|       NULL|      Las Vegas|       Nevada|          us|\n",
      "|        344|   Sunrise Acres-344|2018-07-17|36.163962| -115.11393|0.034444444444444444|    NULL| 7.578947368421052|                NULL|NULL| 41.61538461538461|          10.88125|    NULL|            NULL|              NULL|       NULL|      Las Vegas|       Nevada|          us|\n",
      "|        344|   Sunrise Acres-344|2017-06-17|36.163962| -115.11393|               0.345|    NULL|21.958333333333332|                NULL|NULL|24.083333333333332| 8.752173913043478|    NULL|            NULL|              NULL|       NULL|      Las Vegas|       Nevada|          us|\n",
      "|        358|    Albemarle HS-358|2023-01-23| 38.07657|  -78.50397|                NULL|    NULL|              NULL|0.021000000000000008|NULL|              NULL| 5.291666666666667|    NULL|            NULL|              NULL|       NULL|Charlottesville|     Virginia|          us|\n",
      "|        361|    Tonawanda II-361|2016-10-29|  42.9981|   -78.8993|                NULL|    NULL|              NULL|                NULL|NULL|              NULL|             18.65|    NULL|            NULL|              NULL|       NULL|      Tonawanda|     New York|          us|\n",
      "|        383|           Malta-383|2018-01-16|48.317507|-107.862471|                NULL|    NULL|              NULL|               0.035|NULL|               2.5|1.4500000000000002|    NULL|            NULL|              NULL|       NULL|          Malta|      Montana|          us|\n",
      "|        383|           Malta-383|2024-04-01|48.317507|-107.862471|                NULL|    NULL|              NULL|                NULL|NULL|               4.0|3.8428571428571425|    NULL|            NULL|              NULL|       NULL|          Malta|      Montana|          us|\n",
      "|        410|    West Phoenix-410|2022-05-03|33.483799|-112.142601|  0.3692307692307692|    NULL|           11.1875| 0.04006250000000001|NULL|           32.8125| 9.543750000000001|    NULL|            NULL|              NULL|       NULL|       Glendale|      Arizona|          us|\n",
      "|        442|   North Phoenix-442|2022-12-20|33.560299|-112.066299|                NULL|    NULL|              NULL|0.009173913043478261|NULL| 35.59090909090909|22.863636363636363|    NULL|            NULL|              NULL|       NULL|        Phoenix|      Arizona|          us|\n",
      "|        442|   North Phoenix-442|2017-01-21|33.560299|-112.066299|                NULL|    NULL|              NULL| 0.03938888888888889|NULL| 2.888888888888889| 2.505882352941176|    NULL|            NULL|              NULL|       NULL|        Phoenix|      Arizona|          us|\n",
      "|        442|   North Phoenix-442|2021-12-17|33.560299|-112.066299|                NULL|    NULL|              NULL|0.014095238095238096|NULL|21.583333333333332|12.454166666666667|    NULL|            NULL|              NULL|       NULL|        Phoenix|      Arizona|          us|\n",
      "|        448|Boston - Roxbury-448|2019-10-16|42.329399| -71.082497| 0.32826086956521733|    NULL|14.714285714285714| 0.01827777777777778|NULL|              NULL|10.173913043478262|    NULL|            NULL|               1.0|       NULL|   South Boston|Massachusetts|          us|\n",
      "+-----------+--------------------+----------+---------+-----------+--------------------+--------+------------------+--------------------+----+------------------+------------------+--------+----------------+------------------+-----------+---------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AQI Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aqi\n",
    "\n",
    "def calculate_aqi(pm10, pm25, co, no2, o3, so2):\n",
    "\n",
    "    try:\n",
    "        pollutants = []\n",
    "\n",
    "        pollutants.append((aqi.POLLUTANT_PM10, pm10)) if pm10 else None\n",
    "        pollutants.append((aqi.POLLUTANT_PM25, pm25)) if pm25 else None\n",
    "        pollutants.append((aqi.POLLUTANT_CO_8H, co)) if co else None\n",
    "        pollutants.append((aqi.POLLUTANT_NO2_1H, no2)) if no2 else None\n",
    "        pollutants.append((aqi.POLLUTANT_O3_8H, o3)) if o3 else None\n",
    "        pollutants.append((aqi.POLLUTANT_SO2_1H, so2)) if so2 else None\n",
    "\n",
    "        return(float(aqi.to_aqi(pollutants)))\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "# -------------------------------\n",
    "\n",
    "# register UDF\n",
    "aqi_udf = F.udf(calculate_aqi, T.FloatType())\n",
    "\n",
    "# add AQI column\n",
    "df = df.withColumn('aqi', aqi_udf(F.col('pm10'), F.col('pm25'), F.col('co'), F.col('no2'), F.col('o3'), F.col('so2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aqi_category(aqi_score):\n",
    "    if aqi_score is None:\n",
    "        return None\n",
    "    elif aqi_score <= 50:\n",
    "        return \"Good\"\n",
    "    elif aqi_score <= 100:\n",
    "        return \"Moderate\"\n",
    "    elif aqi_score <= 150:\n",
    "        return \"Unhealthy for Sensitive Groups\"\n",
    "    elif aqi_score <= 200:\n",
    "        return \"Unhealthy\"\n",
    "    elif aqi_score <= 300:\n",
    "        return \"Very Unhealthy\"\n",
    "    elif aqi_score > 300:\n",
    "        return \"Hazardous\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# register UDF\n",
    "aqi_category_udf = F.udf(aqi_category, T.StringType())\n",
    "\n",
    "df = df.withColumn(\"aqi_category\", aqi_category_udf(F.col(\"aqi\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location_id',\n",
       " 'location',\n",
       " 'date',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'co',\n",
       " 'humidity',\n",
       " 'no2',\n",
       " 'o3',\n",
       " 'pm1',\n",
       " 'pm10',\n",
       " 'pm25',\n",
       " 'pressure',\n",
       " 'relativehumidity',\n",
       " 'so2',\n",
       " 'temperature',\n",
       " 'city',\n",
       " 'state',\n",
       " 'country_code',\n",
       " 'aqi',\n",
       " 'aqi_category']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[location_id: bigint, location: string, date: string, lat: double, lon: double, co: double, humidity: double, no2: double, o3: double, pm1: double, pm10: double, pm25: double, pressure: double, relativehumidity: double, so2: double, temperature: double, city: string, state: string, country_code: string, aqi: float, aqi_category: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+---------+-----------+--------------------+--------+------------------+--------------------+----+------------------+------------------+--------+----------------+------------------+-----------+---------------+------------+------------+----+------------+\n",
      "|location_id|            location|      date|      lat|        lon|                  co|humidity|               no2|                  o3| pm1|              pm10|              pm25|pressure|relativehumidity|               so2|temperature|           city|       state|country_code| aqi|aqi_category|\n",
      "+-----------+--------------------+----------+---------+-----------+--------------------+--------+------------------+--------------------+----+------------------+------------------+--------+----------------+------------------+-----------+---------------+------------+------------+----+------------+\n",
      "|        186|Houston Aldine C8...|2023-05-10|  29.9011|   -95.3261|                NULL|    NULL|              NULL|0.025708333333333333|NULL|              NULL|             7.625|    NULL|            NULL|              NULL|       NULL|         Aldine|       Texas|          us|32.0|        Good|\n",
      "|        239|      Alamo Lake-239|2022-03-17|34.243889|-113.558611|                NULL|    NULL|              NULL|               0.046|NULL|               9.6|              4.25|    NULL|            NULL|              NULL|       NULL|         Salome|     Arizona|          us|38.0|        Good|\n",
      "|        259|          Troost-259|2022-10-18| 39.10465|  -94.57055|                NULL|    NULL|11.013636363636364|                NULL|NULL| 34.89090909090909|2.6899999999999995|    NULL|            NULL|1.5818181818181818|       NULL|    Kansas City|    Missouri|          us|31.0|        Good|\n",
      "|        274| Reading Airport-274|2023-10-14|  40.3833|   -75.9686|                NULL|    NULL|              NULL|0.008304347826086956|NULL|              NULL| 4.683333333333333|    NULL|            NULL|              NULL|       NULL|River View Park|Pennsylvania|          us|19.0|        Good|\n",
      "|        288|       Hawthorne-288|2019-09-10|40.733501|-111.871696| 0.11666666666666665|    NULL|12.714285714285714| 0.04671428571428571|NULL|              NULL|18.085714285714285|    NULL|            NULL|               1.0|       NULL|South Salt Lake|        Utah|          us|63.0|    Moderate|\n",
      "|        288|       Hawthorne-288|2017-10-20|40.733501|-111.871696|  0.2820000000000001|    NULL|              19.1|0.022500000000000003|NULL|              NULL| 8.919999999999998|    NULL|            NULL|               1.0|       NULL|South Salt Lake|        Utah|          us|37.0|        Good|\n",
      "|        331|          Lindon-331|2023-01-29|  40.3414|  -111.7136|  0.3166666666666666|    NULL| 7.670833333333334|0.031250000000000014|NULL|               5.0|1.9208333333333334|    NULL|            NULL|              NULL|       NULL|         Lindon|        Utah|          us|26.0|        Good|\n",
      "|        342|       Lancaster-342|2019-07-04|  40.0467|   -76.2833|                NULL|    NULL|              NULL|0.028521739130434785|NULL|              NULL|           11.9125|    NULL|            NULL|              NULL|       NULL|      Lancaster|Pennsylvania|          us|50.0|        Good|\n",
      "|        344|   Sunrise Acres-344|2021-07-01|36.163962| -115.11393| 0.11727272727272725|    NULL|11.545454545454545|                NULL|NULL|28.454545454545453| 7.722727272727272|    NULL|            NULL|              NULL|       NULL|      Las Vegas|      Nevada|          us|32.0|        Good|\n",
      "|        344|   Sunrise Acres-344|2018-07-17|36.163962| -115.11393|0.034444444444444444|    NULL| 7.578947368421052|                NULL|NULL| 41.61538461538461|          10.88125|    NULL|            NULL|              NULL|       NULL|      Las Vegas|      Nevada|          us|45.0|        Good|\n",
      "+-----------+--------------------+----------+---------+-----------+--------------------+--------+------------------+--------------------+----+------------------+------------------+--------+----------------+------------------+-----------+---------------+------------+------------+----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1894400"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model- K means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to address nulls. Options are:\n",
    "\n",
    "    1. drop rows with nulls\n",
    "    2. replace with mean/mode\n",
    "    3. KNN imputation, but requires 3rd party library outside of pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, figuring out how many null values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+---+---+-------+--------+-------+-------+-------+-------+----+--------+----------------+-------+-----------+----+-----+------------+---+------------+\n",
      "|location_id|location|date|lat|lon|     co|humidity|    no2|     o3|    pm1|   pm10|pm25|pressure|relativehumidity|    so2|temperature|city|state|country_code|aqi|aqi_category|\n",
      "+-----------+--------+----+---+---+-------+--------+-------+-------+-------+-------+----+--------+----------------+-------+-----------+----+-----+------------+---+------------+\n",
      "|          0|       0|   0|  0|  0|1678430| 1888744|1608430|1060605|1831132|1420834|   0| 1888858|         1915770|1746648|    1866301|   0|    0|           0|266|         266|\n",
      "+-----------+--------+----+---+---+-------+--------+-------+-------+-------+-------+----+--------+----------------+-------+-----------+----+-----+------------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select(*(count(when(col(c).isNull(), c)).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many columns have over 1,000,000 nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+----+----+-----+--------+-----+-----+-----+-----+----+--------+----------------+-----+-----------+----+-----+------------+----+------------+\n",
      "|location_id|location|date| lat| lon|   co|humidity|  no2|   o3|  pm1| pm10|pm25|pressure|relativehumidity|  so2|temperature|city|state|country_code| aqi|aqi_category|\n",
      "+-----------+--------+----+----+----+-----+--------+-----+-----+-----+-----+----+--------+----------------+-----+-----------+----+-----+------------+----+------------+\n",
      "|       0.00|    0.00|0.00|0.00|0.00|86.77|   97.65|83.15|54.83|94.67|73.46|0.00|   97.65|           99.04|90.30|      96.49|0.00| 0.00|        0.00|0.01|        0.01|\n",
      "+-----------+--------+----+----+----+-----+--------+-----+-----+-----+-----+----+--------+----------------+-----+-----------+----+-----+------------+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "total_rows = df.count()\n",
    "percentage_of_null = df.select([(count(when(col(c).isNull(), c)) / total_rows * 100).alias(c) for c in df.columns])\n",
    "formatted_percentage = percentage_of_null.select([format_number(col(c), 2).alias(c) for c in percentage_of_null.columns])\n",
    "formatted_percentage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping relativehumidity column since it has 99% nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "kmeans_df = df.drop('relativehumidity')\n",
    "# Calculate the mean for each column with potential nulls\n",
    "mean_values = kmeans_df.select([mean(col).alias(col) for col in [\"co\", \"humidity\", \"no2\", \"o3\", \"pm1\", \"pm10\", \"pm25\", \"pressure\",\"so2\", \"temperature\"]]).collect()[0]\n",
    "# Create a dictionary of mean values\n",
    "mean_dict = {col: mean_values[col] for col in mean_values.asDict()}\n",
    "# Fill nulls with the calculated mean values\n",
    "kmeans_df = kmeans_df.fillna(mean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "#assembling relevant columns into a vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"co\", \"humidity\", \"no2\", \"o3\", \"pm1\", \"pm10\", \"pm25\", \"pressure\",\"so2\", \"temperature\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "kmeans_df = assembler.transform(kmeans_df)\n",
    "\n",
    "# fitting the kmeans model for 7 clusters\n",
    "kmeans = KMeans().setK(7).setSeed(1).setFeaturesCol(\"features\").setPredictionCol(\"cluster\")\n",
    "model = kmeans.fit(kmeans_df)\n",
    "\n",
    "predictions = model.transform(kmeans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors (WSSSE) = 665893537.038841\n",
      "+-----------+---------------------+---------------+------------+-------+\n",
      "|location_id|location             |city           |state       |cluster|\n",
      "+-----------+---------------------+---------------+------------+-------+\n",
      "|186        |Houston Aldine C8-186|Aldine         |Texas       |0      |\n",
      "|239        |Alamo Lake-239       |Salome         |Arizona     |0      |\n",
      "|259        |Troost-259           |Kansas City    |Missouri    |0      |\n",
      "|274        |Reading Airport-274  |River View Park|Pennsylvania|0      |\n",
      "|288        |Hawthorne-288        |South Salt Lake|Utah        |0      |\n",
      "|288        |Hawthorne-288        |South Salt Lake|Utah        |0      |\n",
      "|293        |Garden-293           |Anchorage      |Alaska      |0      |\n",
      "|331        |Lindon-331           |Lindon         |Utah        |0      |\n",
      "|342        |Lancaster-342        |Lancaster      |Pennsylvania|0      |\n",
      "|344        |Sunrise Acres-344    |Las Vegas      |Nevada      |0      |\n",
      "|344        |Sunrise Acres-344    |Las Vegas      |Nevada      |0      |\n",
      "|344        |Sunrise Acres-344    |Las Vegas      |Nevada      |0      |\n",
      "|358        |Albemarle HS-358     |Charlottesville|Virginia    |0      |\n",
      "|361        |Tonawanda II-361     |Tonawanda      |New York    |0      |\n",
      "|383        |Malta-383            |Malta          |Montana     |0      |\n",
      "|383        |Malta-383            |Malta          |Montana     |0      |\n",
      "|410        |West Phoenix-410     |Glendale       |Arizona     |0      |\n",
      "|442        |North Phoenix-442    |Phoenix        |Arizona     |0      |\n",
      "|442        |North Phoenix-442    |Phoenix        |Arizona     |0      |\n",
      "|442        |North Phoenix-442    |Phoenix        |Arizona     |0      |\n",
      "+-----------+---------------------+---------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluating model using Within Set Sum of Squared Errors\n",
    "# calculates the sum of the squared euclidian distance of each point in a cluster to it's centroid\n",
    "within_setsum_ofsqured_errors = model.summary.trainingCost\n",
    "print(f\"Within Set Sum of Squared Errors (WSSSE) = {within_setsum_ofsqured_errors}\")\n",
    "\n",
    "# Show the resulting clusters\n",
    "predictions.select(\"location_id\", \"location\", \"city\", \"state\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------------------+--------------------+-------------+-------+\n",
      "|location_id|location                                                        |city                |state        |cluster|\n",
      "+-----------+----------------------------------------------------------------+--------------------+-------------+-------+\n",
      "|358397     |Kettle Falls District office-368999                             |Kettle Falls        |Washington   |1      |\n",
      "|226357     |Murray Jordan River Parkway Trail powered by UTOPIA Fiber-238653|West Jordan         |Utah         |1      |\n",
      "|232584     |200 Eugene Street-244880                                        |Hood River          |Oregon       |1      |\n",
      "|277304     |Heritage-288343                                                 |Louisville          |Colorado     |1      |\n",
      "|234444     |Coeur Rochester Admin Building-246740                           |Lovelock            |Nevada       |1      |\n",
      "|671426     |TFE - Eschinger Ranch-682024                                    |Elk Grove           |California   |1      |\n",
      "|234444     |Coeur Rochester Admin Building-246740                           |Lovelock            |Nevada       |1      |\n",
      "|233503     |friendly fred-245799                                            |Escalon             |California   |1      |\n",
      "|232584     |200 Eugene Street-244880                                        |Hood River          |Oregon       |1      |\n",
      "|233509     |1029-245805                                                     |Gardnerville Ranchos|Nevada       |1      |\n",
      "|77103      |Glenmoor-Fremont-72000                                          |Fremont             |California   |1      |\n",
      "|268320     |EMSonJay-280616                                                 |Gunbarrel           |Colorado     |1      |\n",
      "|326530     |Knoxville-337132                                                |Bolivar             |West Virginia|1      |\n",
      "|233818     |Front-246114                                                    |San Mateo           |California   |1      |\n",
      "|233509     |1029-245805                                                     |Gardnerville Ranchos|Nevada       |1      |\n",
      "|268135     |Harris Family Ranch-280431                                      |Auberry             |California   |1      |\n",
      "|226357     |Murray Jordan River Parkway Trail powered by UTOPIA Fiber-238653|West Jordan         |Utah         |1      |\n",
      "|268320     |EMSonJay-280616                                                 |Gunbarrel           |Colorado     |1      |\n",
      "|268320     |EMSonJay-280616                                                 |Gunbarrel           |Colorado     |1      |\n",
      "|277304     |Heritage-288343                                                 |Louisville          |Colorado     |1      |\n",
      "+-----------+----------------------------------------------------------------+--------------------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster1 = predictions.filter(predictions.cluster == 1)\n",
    "cluster1.select(\"location_id\", \"location\", \"city\", \"state\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+---------+------+-------+\n",
      "|location_id|location         |city     |state |cluster|\n",
      "+-----------+-----------------+---------+------+-------+\n",
      "|7820       |Detroit Lake-7820|Mill City|Oregon|2      |\n",
      "+-----------+-----------------+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster2 = predictions.filter(predictions.cluster == 2)\n",
    "cluster2.select(\"location_id\", \"location\", \"city\", \"state\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------+---------------+----------+-------+\n",
      "|location_id|location                               |city           |state     |cluster|\n",
      "+-----------+---------------------------------------+---------------+----------+-------+\n",
      "|352049     |5986 - Palms El-362651                 |Culver City    |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|65923      |Gibbs Elementary-60921                 |Oronoco        |Minnesota |3      |\n",
      "|1533718    |2527 - Broad Ave El-1503754            |Carson         |California|3      |\n",
      "|230347     |Top of Ohlone Street-242643            |Portola Valley |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352049     |5986 - Palms El-362651                 |Culver City    |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|947229     |4219 - Graham El-1012005               |Florence-Graham|California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|3      |\n",
      "+-----------+---------------------------------------+---------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster3 = predictions.filter(predictions.cluster == 3)\n",
    "cluster3.select(\"location_id\", \"location\", \"city\", \"state\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+-----------------------+----------+-------+\n",
      "|location_id|location                 |city                   |state     |cluster|\n",
      "+-----------+-------------------------+-----------------------+----------+-------+\n",
      "|548        |Grass Valley-548         |Grass Valley           |California|4      |\n",
      "|564        |Durango Complex-564      |Phoenix                |Arizona   |4      |\n",
      "|1867       |Spanish Fork-1867        |Spanish Fork           |Utah      |4      |\n",
      "|1811       |Mira Loma - Van Bure-1811|Glen Avon              |California|4      |\n",
      "|2245       |PECK-2245                |Haysville              |Kansas    |4      |\n",
      "|7636       |Bellingham Pacific-7636  |Bellingham             |Washington|4      |\n",
      "|8810       |Calexico - Ethel Str-8810|Calexico               |California|4      |\n",
      "|230820     |Merced-M St-243116       |Merced                 |California|4      |\n",
      "|272738     |92nd St.-283777          |Florence-Graham        |California|4      |\n",
      "|272740     |AXWG6KH3-283779          |Lynwood                |California|4      |\n",
      "|352094     |1949 - Marlton El-362691 |View Park-Windsor Hills|California|4      |\n",
      "|465        |Broadus-465              |Broadus                |Montana   |4      |\n",
      "|514        |South Phoenix-514        |Phoenix                |Arizona   |4      |\n",
      "|541        |Mesa - Brooks Reserv-541 |Mesa                   |Arizona   |4      |\n",
      "|1811       |Mira Loma - Van Bure-1811|Glen Avon              |California|4      |\n",
      "|1889       |Socorro Hueco C49-1889   |Socorro                |Texas     |4      |\n",
      "|8457       |Madera-City-8457         |Parksdale              |California|4      |\n",
      "|8580       |Hidden Valley-8580       |Maricopa               |Arizona   |4      |\n",
      "|272738     |92nd St.-283777          |Florence-Graham        |California|4      |\n",
      "|410        |West Phoenix-410         |Glendale               |Arizona   |4      |\n",
      "+-----------+-------------------------+-----------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster4 = predictions.filter(predictions.cluster == 4)\n",
    "cluster4.select(\"location_id\", \"location\", \"city\", \"state\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------------------------------+--------------------+----------+-------+\n",
      "|location_id|location                                                   |city                |state     |cluster|\n",
      "+-----------+-----------------------------------------------------------+--------------------+----------+-------+\n",
      "|62703      |Near Round Hill Pines-57702                                |Kingsbury           |Nevada    |5      |\n",
      "|63313      |KAC_PhD-58311                                              |Lake Arrowhead      |California|5      |\n",
      "|63453      |10546 spring valley drive-58451                            |New Kingman-Butler  |Arizona   |5      |\n",
      "|68840      |Aspen East End-63838                                       |Aspen               |Colorado  |5      |\n",
      "|70097      |Payson Mountain View Hospital powered by UTOPIA Fiber-65095|Payson              |Utah      |5      |\n",
      "|73545      |Descanso Air-68543                                         |Spanish Springs     |Nevada    |5      |\n",
      "|1289787    |YOSE @ Tuolumne Meadows-1260007                            |Yosemite Valley     |California|5      |\n",
      "|63793      |Upper Johnson Lane-58791                                   |Johnson Lane        |Nevada    |5      |\n",
      "|64655      |Oaks-59653                                                 |East Millcreek      |Utah      |5      |\n",
      "|234803     |BLS- Augusta Dr.-247099                                    |Arnold              |California|5      |\n",
      "|67262      |MC^2-62260                                                 |Truckee             |California|5      |\n",
      "|227523     |APCD Fairplay-239819                                       |Fairplay            |Colorado  |5      |\n",
      "|1131686    |Bridlegate-1133513                                         |Ken Caryl           |Colorado  |5      |\n",
      "|64155      |Pine Dr.-59153                                             |Kingsbury           |Nevada    |5      |\n",
      "|64514      |Olympic Donner Lake-59512                                  |Truckee             |California|5      |\n",
      "|230167     |Mazama Village - Crater Lake-242463                        |Shady Cove          |Oregon    |5      |\n",
      "|234667     |1765 Sequoia Avenue-246963                                 |Sunnyside-Tahoe City|California|5      |\n",
      "|63400      |WSU Facilities Management-58398                            |South Ogden         |Utah      |5      |\n",
      "|68839      |Silver Summit-63837                                        |Snyderville         |Utah      |5      |\n",
      "|358391     |Kettle Falls Middle School-368992                          |Kettle Falls        |Washington|5      |\n",
      "+-----------+-----------------------------------------------------------+--------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster5 = predictions.filter(predictions.cluster == 5)\n",
    "cluster5.select(\"location_id\", \"location\", \"city\", \"state\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------+---------------+----------+-------+\n",
      "|location_id|location                               |city           |state     |cluster|\n",
      "+-----------+---------------------------------------+---------------+----------+-------+\n",
      "|6912       |Indio-29 Palms-6912                    |Indio          |California|6      |\n",
      "|8692       |Mammoth Lakes-8692                     |Mammoth Lakes  |California|6      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|6      |\n",
      "|1881       |NCORE - Bishop-1881                    |Bishop         |California|6      |\n",
      "|352049     |5986 - Palms El-362651                 |Culver City    |California|6      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|6      |\n",
      "|1189       |Keeler-1189                            |Lone Pine      |California|6      |\n",
      "|1236054    |Eastmont-1224579                       |Mill Creek     |Washington|6      |\n",
      "|352049     |5986 - Palms El-362651                 |Culver City    |California|6      |\n",
      "|8583       |Hollister AMS-8583                     |Ridgemark      |California|6      |\n",
      "|8583       |Hollister AMS-8583                     |Ridgemark      |California|6      |\n",
      "|352882     |743 S. MARK ST. TULARE CA-363483       |Tulare         |California|6      |\n",
      "|352049     |5986 - Palms El-362651                 |Culver City    |California|6      |\n",
      "|947229     |4219 - Graham El-1012005               |Florence-Graham|California|6      |\n",
      "|352049     |5986 - Palms El-362651                 |Culver City    |California|6      |\n",
      "|352049     |5986 - Palms El-362651                 |Culver City    |California|6      |\n",
      "|947303     |7863 - Woodcrest El-964747             |Westmont       |California|6      |\n",
      "|8692       |Mammoth Lakes-8692                     |Mammoth Lakes  |California|6      |\n",
      "|8673       |Lee Vining-8673                        |Bridgeport     |California|6      |\n",
      "|352092     |5014 - Loyola Village Elementary-362697|Marina del Rey |California|6      |\n",
      "+-----------+---------------------------------------+---------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster6 = predictions.filter(predictions.cluster == 6)\n",
    "cluster6.select(\"location_id\", \"location\", \"city\", \"state\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can plot results in 2D using PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the AQI score based on other features: location (lat/lon), weather, but not including pm25. Seeing which features are most important for prediciting AQI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_df = df.drop('relativehumidity', '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at Average PM2.5 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of average pm25 by date. this is averaged over all locations\n",
    "df_pm25 = df.groupBy('date').agg(F.avg('pm25').alias('pm25'))\n",
    "_pm25 = df_pm25.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "_pm25['date'] = pd.to_datetime(_pm25['date'])\n",
    "\n",
    "# sort by date\n",
    "_pm25 = _pm25.sort_values('date')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(_pm25['date'], _pm25['pm25'], marker='o', linestyle='-')\n",
    "\n",
    "plt.title('PM2.5 Values in US')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average PM2.5 (µg/m³)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into  training and tests sets, create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "df_model = df\n",
    "\n",
    "# feature engineering: extract date parts\n",
    "df_model = df_model.withColumn(\"year\", F.year(\"date\"))\n",
    "df_model = df_model.withColumn(\"month\", F.month(\"date\"))\n",
    "df_model = df_model.withColumn(\"day\", F.dayofmonth(\"date\"))\n",
    "df_model = df_model.withColumn(\"day_of_week\", F.dayofweek(\"date\"))\n",
    "\n",
    "# creating feature vectors\n",
    "assembler = VectorAssembler(inputCols=[\"year\", \"month\", \"day\", \"day_of_week\"], outputCol=\"features\")\n",
    "df_model = assembler.transform(df_model)\n",
    "\n",
    "# split into test and training sets based on 80/20\n",
    "train_df, test_df = df_model.randomSplit([0.8, 0.2])\n",
    "\n",
    "# split into test and training sets based on date\n",
    "# train_df = df_model.filter(F.col('date') < '2023-01-01')\n",
    "# test_df = df_model.filter(F.col('date') >= '2023-01-01')\n",
    "\n",
    "# create a linear regression model\n",
    "lr_model = LinearRegression(featuresCol='features', labelCol='pm25').fit(train_df)\n",
    "\n",
    "# print the coefficients and intercept for linear regression\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df\n",
    "\n",
    "# converting date column to timestamp \n",
    "df_model = df.withColumn('timestamp', F.unix_timestamp(F.col('date'), 'yyyy-MM-dd'))\n",
    "\n",
    "# creating feature vectors\n",
    "assembler = VectorAssembler(inputCols=[\"timestamp\"], outputCol=\"features\")\n",
    "df_model = assembler.transform(df_model)\n",
    "\n",
    "# split into test and training sets based on date\n",
    "train_df = df_model.filter(F.col('date') < '2023-01-01')\n",
    "test_df = df_model.filter(F.col('date') >= '2023-01-01')\n",
    "\n",
    "# create a linear regression model\n",
    "lr_model = LinearRegression(featuresCol='features', labelCol='pm25').fit(train_df)\n",
    "\n",
    "# print the coefficients and intercept for linear regression\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on training data\n",
    "train_predictions = lr_model.transform(train_df)\n",
    "\n",
    "# predictions on testing data\n",
    "test_predictions = lr_model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"pm25\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# RMSE for training\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "\n",
    "# RMSE for testing\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Testing RMSE:\", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "Our model fits in the underfitting side of the fitting graph. Our linear regression model is on the more simple side, with only one parameter reading vs. timestamp as the input of trying to predict future values. We started off with pm values across many stations, but averaged these across the entire US and across each day as an input to our model, which simplified the model. Unforunately, the values in our dataset only go a couple of years back. Because it is more simple due to these reasons, the model is prone to underfitting and not being as robust as if we had added more time points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Models\n",
    "\n",
    "The next models we are thinking of will utilize more of the parameters we have in our dataset. We can apply k-means clustering to obtain clusters of places with similar levels/patterns of pollution based on features. This would be interesting to investigate as we could look into areas of best and worst pollution in the US. We could also run PCA on these parameter features for dimensionality reduction on our dataset as we have many parameters of data to pool from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
